{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Conv2d, MaxPool2d, Flatten\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "from src.data.ucmerced_dataset import UcMercedDataset\n",
    "from src.settings import DATA_DIRECTORY\n",
    "from src.settings import TRAIN_DATA_DIRECTORY, TEST_DATA_DIRECTORY\n",
    "from src.evaluation import evaluate_anmrr, evaluate_loss\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def soft_pn(a, p, n):\n",
    "    def dist(x, y):\n",
    "        return torch.linalg.norm((x - y))\n",
    "    \n",
    "    dist_p = dist(a, p)\n",
    "    dist_n1 = dist(a, n)\n",
    "    dist_n2 = dist(p, n)\n",
    "\n",
    "    min_n_dist = torch.minimum(dist_n1, dist_n2)\n",
    "\n",
    "    pos_exp = dist_p.exp()\n",
    "    min_n_exp = min_n_dist.exp()\n",
    "\n",
    "    l = (pos_exp / (min_n_exp + pos_exp)).pow(2) + (min_n_exp / (min_n_exp + pos_exp) - 1).pow(2)\n",
    "    return l.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "train_dataset = UcMercedDataset(TRAIN_DATA_DIRECTORY, image_size, train=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=40,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = UcMercedDataset(TEST_DATA_DIRECTORY, image_size, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_training_model_layers(model, training: bool, up_to_index: int):\n",
    "    i = 0\n",
    "    for child in model.children():\n",
    "        if i > up_to_index:\n",
    "            break\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = training\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import Sequential, Linear, ReLU, Conv2d, MaxPool2d, Flatten, LocalResponseNorm, BatchNorm2d, LeakyReLU, Tanh\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "act = LeakyReLU()\n",
    "# act = Tanh()\n",
    "\n",
    "# model = Sequential(\n",
    "#     Conv2d(3, 32, (3, 3)),\n",
    "#     BatchNorm2d(32),\n",
    "#     LeakyReLU(),\n",
    "#     MaxPool2d(2, 2),\n",
    "\n",
    "#     Conv2d(32, 32, (3, 3)),\n",
    "#     BatchNorm2d(32),\n",
    "#     LeakyReLU(),\n",
    "#     MaxPool2d(2, 2),\n",
    "\n",
    "#     Conv2d(32, 64, (3, 3)),\n",
    "#     BatchNorm2d(64),\n",
    "#     LeakyReLU(),\n",
    "#     MaxPool2d(2, 2),\n",
    "\n",
    "#     Conv2d(64, 64, (3, 3)),\n",
    "#     BatchNorm2d(64),\n",
    "#     LeakyReLU(),\n",
    "#     MaxPool2d(2, 2),\n",
    "    \n",
    "#     Conv2d(64, 128, (3, 3)),\n",
    "#     BatchNorm2d(128),\n",
    "#     LeakyReLU(),\n",
    "#     MaxPool2d(2, 2),\n",
    "\n",
    "#     Flatten(),\n",
    "#     Linear(4608, 100),\n",
    "#     LeakyReLU(),\n",
    "#     Linear(100, 100),\n",
    "#     # Tanh()\n",
    "# )\n",
    "\n",
    "model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(512, 100, bias=True)\n",
    "set_training_model_layers(model, False, 8)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "criterion = torch.nn.TripletMarginLoss()\n",
    "# criterion = soft_pn\n",
    "\n",
    "def evaluate_model(model, train: DataLoader, test: DataLoader, criterion):\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        test_loss = evaluate_loss(model, test, criterion)\n",
    "\n",
    "        train_anmrr = evaluate_anmrr(model, train_dataloader, euclidean_distances)\n",
    "        test_anmrr = evaluate_anmrr(model, test_dataloader, euclidean_distances)\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    return test_loss, train_anmrr, test_anmrr\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "t = trange(30)\n",
    "for epoch in t:\n",
    "    loss_sum = 0.0\n",
    "    if epoch == 10:\n",
    "      set_training_model_layers(model, True, 8)\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        anchors = sample_batched['a'].cuda()\n",
    "        positives = sample_batched['p'].cuda()\n",
    "        negatives = sample_batched['n'].cuda()\n",
    "        a = model(anchors)\n",
    "        p = model(positives)\n",
    "        n = model(negatives)\n",
    "        loss = criterion(a, p, n)\n",
    "        loss.backward()\n",
    "        loss_sum += float(loss)\n",
    "        optim.step()\n",
    "        t.set_description(f\"Batch: {i_batch}\")\n",
    "    del a\n",
    "    del p\n",
    "    del n\n",
    "    del loss\n",
    "    \n",
    "    train_loss = loss_sum / len(train_dataloader)\n",
    "\n",
    "    test_loss, train_anmmr, test_anmrr = evaluate_model(model, train_dataloader, test_dataloader, criterion)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('ANMRR/train', train_anmmr, epoch)\n",
    "    writer.add_scalar('ANMRR/test', test_anmrr, epoch)\n",
    "    t.set_description(f\"Epoch: {epoch}, Train loss: {train_loss}, Test loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "embeddings = []\n",
    "classes = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "        anchors = sample_batched['a'].cuda()\n",
    "        y = sample_batched['a_y']\n",
    "        classes.append(y.cpu().numpy())\n",
    "        anchor_paths = sample_batched['path']\n",
    "        paths.extend(anchor_paths)\n",
    "        a = model(anchors).cpu().numpy()\n",
    "        embeddings.append(a)\n",
    "\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    classes = np.concatenate(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "distances = euclidean_distances(embeddings)\n",
    "if(len(classes.shape) < 2):\n",
    "    classes = classes[:, None]\n",
    "\n",
    "paths = np.array(paths).squeeze()\n",
    "rankings = np.argsort(distances, axis=1)\n",
    "selected_images = paths[rankings]\n",
    "\n",
    "cols = 6\n",
    "rows = 6\n",
    "\n",
    "for label, name in test_dataset.label_name_mapping.items():\n",
    "    \n",
    "    indices_with_class = np.argwhere(classes == label)[:, 0].squeeze()\n",
    "    example_query_index = np.random.choice(indices_with_class)\n",
    "\n",
    "    query_image_path = paths[example_query_index]\n",
    "    example_query = selected_images[example_query_index, :].squeeze()\n",
    "\n",
    "    query_image = io.imread(query_image_path)\n",
    "    plt.imshow(query_image)\n",
    "    query_image_name = os.path.split(query_image_path)[1]\n",
    "    plt.title(f\"Query: {query_image_name}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    fig=plt.figure(figsize=(20, 20))\n",
    "    for i in range (cols * rows):\n",
    "        path = example_query[i]\n",
    "        image = io.imread(path)\n",
    "        fig.add_subplot(rows, cols, i+1)\n",
    "        plt.title(os.path.split(path)[1])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.imshow(image)\n",
    "    fig.suptitle(f\"Response to query: {query_image_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "with torch.no_grad():\n",
    "    test_anmrr, class_anmrr = evaluate_anmrr(model, test_dataloader, euclidean_distances, class_mean=True)\n",
    "model.train(True)\n",
    "anmrr_with_labels = [(train_dataset.label_name_mapping[c], value) for c, value in class_anmrr]\n",
    "anmrr_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(zip(*anmrr_with_labels))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
